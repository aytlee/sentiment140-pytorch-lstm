{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMymJ4+YxJ1jsfIsby+r5ox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aytlee/sentiment140-pytorch-lstm/blob/main/sentiment140_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "The objective of this notebook is to gain a better understanding of how to preprocess and apply a LSTM model for sentiment analysis using the Sentiment 140 dataset. This dataset was downloaded from Kaggle for ease of use, and contains 1.6M tweets that were extracted using Twitter's API. Additional information and the original dataset can be found [here](http://help.sentiment140.com/home). The original training dataset is unique in that it was automatically labeled rather than manually labeled. The notebook follows the code/tutorial from:\n",
        "\n",
        "1. [LSTM Text Classification Using PyTorch](https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0) by Raymond Chang\n",
        "2. [Upgraded Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis) by Ben Trevett \n"
      ],
      "metadata": {
        "id": "p4HRfphcpwll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries \n",
        "\n",
        "First, load the dataset and import all of the required libraries. While importing libraries, it was necessary to specify a version of torchtext in order to import the torchtext.legacy module. "
      ],
      "metadata": {
        "id": "puNsurrE25JQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "8bYbyrSEopH-",
        "outputId": "3866e013-e316-4bfd-9243-3b3d8f3c5005"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-02a16673-0391-411d-8201-f1f6837773de\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-02a16673-0391-411d-8201-f1f6837773de\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# # Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Upload kaggle API key file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "#! kaggle competitions download -c tabular-playground-series-apr-2022\n",
        "! mkdir train\n",
        "! kaggle datasets download -d kazanova/sentiment140 -p /content/train/ --unzip\n",
        "#! unzip tabular-playground-series-apr-2022.zip -d train"
      ],
      "metadata": {
        "id": "DHtNQzcu0uVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f3e2d3-b9f4-45ae-a95b-2b127c774b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sentiment140.zip to /content/train\n",
            " 69% 56.0M/80.9M [00:00<00:00, 108MB/s] \n",
            "100% 80.9M/80.9M [00:00<00:00, 134MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seems to be an issue with intalling torchtext without specifying version\n",
        "# cannot import torchtext.legacy module without running into issues \n",
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8d-IYclFRX6",
        "outputId": "e733cfb6-28a4-478c-d76d-f201989d743d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn import model_selection \n",
        "\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "#import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "#from torch.utils.data import Dataset, DataLoader \n",
        "\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from multiprocessing import cpu_count\n",
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "Skh5gDPZ6AJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT = data.Field()\n",
        "# LABEL = data.LabelField(dtype = torch.long)\n",
        "# legacy_train, legacy_test = datasets.IMDB.splits(TEXT, LABEL)  # datasets here refers to torchtext.legacy.datasets"
      ],
      "metadata": {
        "id": "ZVZzBSHrDC9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkdTnnWEHmLv",
        "outputId": "084fb0ce-41a8-41e1-f45f-eb475c0f0b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration of the training dataset \n",
        "\n",
        "According to the metadata, there are 6 columns consisting of the target, id, date, flag, user and text for the tweet. Targets are labeled 0 (negative), 2 (neutral) and 4 (positive). The training dataset only includes the negative and positive tweets, while the test dataset (found on the Stanford website) also includes the neutral tweets. \n",
        "\n",
        "In addition, the training dataset set only has the value \"NO_QUERY\" for the column flag, while the test dataset seems to have multiple values for the column. The \"flag\" column seems to denote some keyword that is associated with the tweet. As this notebook is meant mainly to familiarize myself with NLP using PyTorch and LSTM, I will not be using the test dataset and instead only conducting the model on a split training and validation set. \n",
        "\n"
      ],
      "metadata": {
        "id": "FT5PSzl53fQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(r'training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)"
      ],
      "metadata": {
        "id": "nysa-85-4mjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8UX655s4woC",
        "outputId": "4df48ae6-b259-4007-c09c-295a1cfd4465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600000 entries, 0 to 1599999\n",
            "Data columns (total 6 columns):\n",
            " #   Column  Non-Null Count    Dtype \n",
            "---  ------  --------------    ----- \n",
            " 0   0       1600000 non-null  int64 \n",
            " 1   1       1600000 non-null  int64 \n",
            " 2   2       1600000 non-null  object\n",
            " 3   3       1600000 non-null  object\n",
            " 4   4       1600000 non-null  object\n",
            " 5   5       1600000 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add in the column headings\n",
        "train_data.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
        "\n",
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5sAGZumx6rU_",
        "outputId": "c4a459c7-8e0b-4d81-af89-bc5070254444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   target          id                          date      flag  \\\n",
              "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
              "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
              "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
              "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
              "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
              "\n",
              "              user                                               text  \n",
              "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
              "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
              "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
              "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
              "4           Karoli  @nationwideclass no, it's not behaving at all....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8da44826-b8a2-45bc-9bb8-1015992e5775\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8da44826-b8a2-45bc-9bb8-1015992e5775')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8da44826-b8a2-45bc-9bb8-1015992e5775 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8da44826-b8a2-45bc-9bb8-1015992e5775');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the proportion of the targets in the dataset\n",
        "print(train_data['target'].value_counts(normalize=True))\n",
        "# Apparently there are no neutral comments in the dataset, the dataset is balanced\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLJdBcTq6tr5",
        "outputId": "ad2c4246-d30a-46df-e8d4-ce22ab353256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    0.5\n",
            "4    0.5\n",
            "Name: target, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['flag'].value_counts()\n",
        "# No flag in this dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeHMVwmL8w8n",
        "outputId": "d04908ab-6820-4ada-e359-e087d8821ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NO_QUERY    1600000\n",
              "Name: flag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of unique users in this dataset:', train_data['user'].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOwX0UJWAAQp",
        "outputId": "5315de5a-ff0c-4de0-cb17-a48d446811f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique users in this dataset: 659775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at a sample of the negative tweets \n",
        "neg_tweets = train_data.query('target==0')[['text']].sample(10)\n",
        "\n",
        "for i in neg_tweets.iterrows():\n",
        "  print(neg_tweets.loc[i[0], 'text'], '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve0JM09UAFT2",
        "outputId": "aed97845-7883-4edc-e766-11dfa4a81d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@caguilar69 No, i work  \n",
            "\n",
            "PS. Creeping through the grade 8s' Claremont albums on Facebook make me feel nostalgic... whatever happened to the &quot;Ant Hill Trio&quot;, lol.  \n",
            "\n",
            "Having trouble staying asleep. It's so hot and I'm exhausted  \n",
            "\n",
            "@OfficialJoBros u guys are going to like dis i went 2 3 or 4 stores and they are sold out of your new album!!!NOT COOL  \n",
            "\n",
            "Is sick of having no voice!!!  \n",
            "\n",
            "@dazzleme18 heyy i just put the final product together and it took like the last hour  lol i dunno if i can do it again  \n",
            "\n",
            "Soooo tired...not sure I will get any writing done.  \n",
            "\n",
            "@meerski loll I just saw this but I bet he did.  eww he needs to make it &quot;Every WOMAN&quot; \n",
            "\n",
            "@restartt they are expensive  \n",
            "\n",
            "I just want to watch my Naruto Shippuuden missed episodes.  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at a sample of the positive tweets \n",
        "pos_tweets = train_data.query('target==4')[['text']].sample(10)\n",
        "\n",
        "for i in pos_tweets.iterrows():\n",
        "  print(pos_tweets.loc[i[0], 'text'], '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8HFs-J5BJLn",
        "outputId": "a1255709-d1ee-4597-e0fa-2d9daa84d8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@TastesLikePain do you think you'd be able to cme? We can party with Miley  \n",
            "\n",
            "@Mr_Ant704 IDK yet but it should include a beach tho  \n",
            "\n",
            "So its Superstar Sunday? @pradeepto is one superstar I know   #sunday \n",
            "\n",
            "@kingbritt I love my dentist  hope all is well wid ya  \n",
            "\n",
            "@JonathanRKnight oh Jon u can watch me any time u like babe  xx \n",
            "\n",
            "NEW TOY!!!!!! Yessssss!!!! I just might sleep in my car tonight!!!  \n",
            "\n",
            "Im inspired much by everything that is happening.    One choice, one way.    ohh .. this is just my first yime ..  plz. excuse me  tnx \n",
            "\n",
            "got into my apartment without keys and drunk ya!  \n",
            "\n",
            "@rhettroberts @JamesMW78 that's a cruel &amp; tantalizing dilemma! But this is false dichotomy, I can drive with someone seated on my laps  \n",
            "\n",
            "Happy Birthday to my son  Welcome to adulthood!! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relabel the positive tweets \n",
        "train_data['target'] = train_data['target'].map({4:1, 0:0})"
      ],
      "metadata": {
        "id": "MzL_l_lLrCFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iOiLvtfrJmR",
        "outputId": "e3dfd01c-60f7-4349-b4dc-e7adfdb45a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    800000\n",
              "1    800000\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the Stanford paper and website, the original analysis of the tweets was done automatically rather than manually reviewed. The tweets were labeled as either negative or positive based on a query term in the tweet. Their training data also used emoticons and distant supervision machine learning in order to determine the sentiment. In this notebook, we will use torchtext and a LSTM in order to process and determine the tweet sentiment. "
      ],
      "metadata": {
        "id": "5qwdNrbD99bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process and set up the text data for PyTorch\n"
      ],
      "metadata": {
        "id": "V3sIfbXYAt9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below declares the Fields that will be used in the model. The Field specifies the data type, holds a Vocab object and determines how the data will be converted to a Tensor. I will also split the dataset using train_test_split and writing the files to a CSV file to be loaded into a TabularDataset. TabularDataset will read in columns from TSV, CSV, jSON files and convert them to a Dataset object. "
      ],
      "metadata": {
        "id": "f_u9VCrSLV4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(tokenize='spacy', \n",
        "                  tokenizer_language='en_core_web_sm', \n",
        "                  include_lengths=True,\n",
        "                  batch_first=True)\n",
        "\n",
        "LABEL = data.LabelField(dtype=torch.float, batch_first=True)\n",
        "\n",
        "fields = [('target', LABEL), ('text', TEXT)]"
      ],
      "metadata": {
        "id": "SVaui0kbAXG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_valid = train_test_split(train_data[['target', 'text']], test_size=0.2, random_state=123)"
      ],
      "metadata": {
        "id": "HP5Km3LNQ_Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_csv(r'df_train.csv', index=False)\n",
        "df_valid.to_csv(r'df_valid.csv', index=False)"
      ],
      "metadata": {
        "id": "Eaxr91XYaNY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsx-16IdLcBp",
        "outputId": "cbab2e98-f7e8-46ad-b652-fae6c23001ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_folder = os.getcwd()\n",
        "trainds, validds = data.TabularDataset.splits(path=source_folder, \n",
        "                                          format='CSV',\n",
        "                                          train='df_train.csv', \n",
        "                                          validation='df_valid.csv',\n",
        "                                          fields=fields,\n",
        "                                          skip_header=True)"
      ],
      "metadata": {
        "id": "ykaWCccxaNy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some of the variables in the tabular dataset to see what they look like \n",
        "print(vars(trainds[0]))\n",
        "print(vars(validds[-1]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLcQM2bOmhSE",
        "outputId": "266ca4ca-7254-4987-9229-bfb34c5c1a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'target': '1', 'text': ['withholding', 'information', 'am', 'I', 'Shae', '?', 'YUP', ' ']}\n",
            "{'target': '1', 'text': ['Hi', ',', 'my', 'name', \"'s\", 'Doug', '...', 'my', 'owner', 'SQUIRREL!', '...', 'Hi', ',', 'my', 'name', \"'s\", 'Doug', '...']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the Dataset object, we can build a BucketIterator object using torchtext. Essentially, TabularDataset and BucketIterator are like the Dataset and DataLoader objects used in torch. However, BucketIterator will sort the data so that sequences with similar lengths are grouped together within a batch. This optimizes the batches when being input into the model and reduces the amount of padding needed. \n",
        "\n",
        "According to [this](https://www.analyticsvidhya.com/blog/2021/09/sentiment-analysis-with-lstm-and-torchtext-with-code-and-explanation/) article, the build_vocab function will generate the Vocab object by tokenizing the words, and provide a tensor of the token IDs for each sequence. If we take a look at the items in each batch from the BucketIterator, we see that it returns a Tensor with numerical values rather than word tokens. Although it's not done in this model, we can also pass pre-trained word embeddings to build_vocab. This is done in Ben Trevett's Upgraded Sentiment Analysis tutorial and is explained in more detail there. "
      ],
      "metadata": {
        "id": "ohv7fDEeOOLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(trainds, min_freq = 3)\n",
        "LABEL.build_vocab(trainds) "
      ],
      "metadata": {
        "id": "NCeDCZq3Dvea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BucketIterator is an iterator that batches examples of similar lengths together \n",
        "# Minimizes the amount of padding and shuffles batches for each new epoch \n",
        "\n",
        "train_iter, valid_iter = data.BucketIterator.splits(\n",
        "    (trainds, validds), \n",
        "    batch_size=64,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort=False,\n",
        "    sort_within_batch=True,\n",
        "    device=device)"
      ],
      "metadata": {
        "id": "bFceDkw9akyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over bucket iterator to see what each batch and looks like \n",
        "print('PyTorchText BucketIterator\\n')\n",
        "\n",
        "for batch_no, batch in enumerate(train_iter):\n",
        "    text, batch_len = batch.text\n",
        "    print(text, batch_len)\n",
        "    print(batch.target)\n",
        "    break\n",
        "\n",
        "  # Only look at first batch. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzRsnEy2Rxei",
        "outputId": "7ebef7fc-19d5-4426-a188-7d5f8989a194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorchText BucketIterator\n",
            "\n",
            "tensor([[ 517,   35,   53,  ..., 5356,   11, 2707],\n",
            "        [ 434,  310, 1199,  ...,    2,    2,    2],\n",
            "        [   0,  514,    8,  ..., 9988,    5,    0],\n",
            "        ...,\n",
            "        [   0,  767,   10,  ...,  355, 1231,    1],\n",
            "        [  72,  246, 1106,  ...,    5,    0,    1],\n",
            "        [2127,  252,    2,  ...,  225,    3,    1]], device='cuda:0') tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
            "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
            "        18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
            "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17], device='cuda:0')\n",
            "tensor([1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 1., 0., 0.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the below dictionary, you can see that stoi creates a dictionary with the token and the corresponding token ID \n",
        "# unknown and padding are 0 and 1 respectively and all other words come after \n",
        "\n",
        "# TEXT.vocab.stoi.items()"
      ],
      "metadata": {
        "id": "vqIAdn3wGWQh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model below consists of:\n",
        "- an embedding layer \n",
        "- a bidirectional LSTM  \n",
        "- dropout \n",
        "- fully connected layer \n",
        "\n",
        "The text is input into an embedding layer to create the word embeddings, then they are packed and padded before being input in the bidirectional LSTM. The last outputs from the forward and backward pass of the LSTM is then passed to the dropout layer (for regularization) before being passed to the fully connected layer. The model uses BCELoss so it's necessary to pass the outputs of the forward layer through a sigmoid function."
      ],
      "metadata": {
        "id": "DdXR7BupchGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, output_dim, hidden_dim, input_dim, embedding_dim, num_layers, dropout):\n",
        "    super().__init__() # should this just be super().__init__()?\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim) \n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim, \n",
        "                        hidden_size=hidden_dim,\n",
        "                        num_layers=num_layers,\n",
        "                        batch_first=True,\n",
        "                        bidirectional=True)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, text, text_len):\n",
        "\n",
        "    text_emb = self.embedding(text)\n",
        "\n",
        "    # Packs a Tensor containing padded sequences of variable length \n",
        "    packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    packed_output, (hidden, cell) = self.lstm(packed_input) \n",
        "    # unpack the padded sequence \n",
        "    output, output_len = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "    # out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "    hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "    text_fea = self.drop(hidden)\n",
        "\n",
        "    text_fea = self.fc(text_fea)\n",
        "    text_fea = torch.squeeze(text_fea, 1)\n",
        "    text_out = torch.sigmoid(text_fea)\n",
        "\n",
        "    return text_out"
      ],
      "metadata": {
        "id": "pScdkURFLunQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5 \n"
      ],
      "metadata": {
        "id": "IqKHLEqMYmen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(OUTPUT_DIM, HIDDEN_DIM, INPUT_DIM, EMBEDDING_DIM, \n",
        "             N_LAYERS, DROPOUT).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss() \n",
        "#criterion = criterion.to(device) # criterion needs to be sent to GPU? \n",
        "# Check if packed inputs need to be sent to GPU?"
      ],
      "metadata": {
        "id": "X9JIgtcVl42y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_accuracy(preds, y):\n",
        "  '''\n",
        "  Returns accuracy per batch\n",
        "  '''\n",
        "\n",
        "  rounded_preds = torch.round(preds) \n",
        "  correct = (rounded_preds == y).float() # convert to float for division\n",
        "  acc = correct.sum()/len(correct)\n",
        "  return acc \n"
      ],
      "metadata": {
        "id": "BPtJ68dkzf05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A note for the train and eval functions below, the text_len variable must be a CPU tensor and not a GPU tensor like text or labels. As `train_iter` and `val_iter` were set up to be on GPU, the text and label variables will be on the GPU already. "
      ],
      "metadata": {
        "id": "KUqNdk8-GHfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: The text_len variable must be a CPU tensor and not a GPU tensor like the other inputs. \n",
        "\n",
        "def train(mode, iterator, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch in iterator:\n",
        "    labels = batch.target\n",
        "    text, text_len = batch.text \n",
        "    \n",
        "    text_len = text_len.cpu()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(text, text_len)\n",
        "    loss = criterion(predictions, labels)\n",
        "    acc = binary_accuracy(predictions, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "jtUlI4D1jHh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_len = batch.text\n",
        "            labels = batch.target\n",
        "\n",
        "            # labels.to(device)\n",
        "            # text.to(device)\n",
        "            # text_len.to(device)\n",
        "\n",
        "            text_len = text_len.cpu()\n",
        "            predictions = model(text, text_len)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "OAuOIjY2VulV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "U1Hf7BlUOtaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "  \n",
        "  train_loss_list.append(train_loss)\n",
        "  valid_loss_list.append(valid_loss)\n",
        "\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time) \n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'first-draft-model.pt')\n",
        "\n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "  print(f'\\t Val.Loss: {valid_loss:.3f} | Val.Acc: {valid_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0-VscLgDjMz",
        "outputId": "71e2fabf-003f-44c5-b310-67152ad05e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 8m 11s\n",
            "\tTrain Loss: 0.375 | Train Acc: 83.23%\n",
            "\t Val.Loss: 0.337 | Val.Acc: 85.24%\n",
            "Epoch: 02 | Epoch Time: 8m 10s\n",
            "\tTrain Loss: 0.303 | Train Acc: 87.12%\n",
            "\t Val.Loss: 0.330 | Val.Acc: 85.71%\n",
            "Epoch: 03 | Epoch Time: 8m 11s\n",
            "\tTrain Loss: 0.264 | Train Acc: 89.02%\n",
            "\t Val.Loss: 0.336 | Val.Acc: 85.67%\n",
            "Epoch: 04 | Epoch Time: 8m 10s\n",
            "\tTrain Loss: 0.230 | Train Acc: 90.60%\n",
            "\t Val.Loss: 0.351 | Val.Acc: 85.24%\n",
            "Epoch: 05 | Epoch Time: 8m 9s\n",
            "\tTrain Loss: 0.202 | Train Acc: 91.84%\n",
            "\t Val.Loss: 0.377 | Val.Acc: 85.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After 5 epochs, the accuracy for the training dataset went from 83% to 91%; however, the validation accuracy stayed around 85%, and the accuracy does drop slightly by the last epoch. The accuracy seems to imply that the data is being overfit as it continues. Another metric that can be looked at is the classification report to see the precision and recall of the model. \n",
        "\n",
        "A few things that could be tried to see if the metrics can improve:\n",
        "- Use pre-trained word embeddings. Glove has word embeddings for tweets specifically.\n",
        "- Change the drop out value to see if increased regularization will reduce overfitting \n",
        "- Process the data more to remove certain values, e.g. usernames, links, that might not contribute to the sentiment analysis\n",
        "\n",
        "It would also be interesting to try a simpler model, e.g. bag of words or TFIDF, to see if these models would still output similar results. Each epoch took about 8 minutes to run for this bidirectional LSTM, so if a simpler model could provide similar results, then that could lead to faster computational and processing times. "
      ],
      "metadata": {
        "id": "WYG-UZKiYhk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determining the Last Time Step of the Backward and Forward Pass\n",
        "\n",
        "Checked to see if `output_forward` matched `output[:, -1, :self.dimension]` or `hidden[-2, :, :]`. Found that the majority of the tensor outputs were the same but that there was one difference in certain batches with `output[:, -1, :self.dimension]`. \n",
        "\n",
        "out_forward: \n",
        "\n",
        "```\n",
        "tensor([[ 0.0341, -0.0997, -0.0522,  ..., -0.0227,  0.0251,  0.3660],\n",
        "        [ 0.0488,  0.0952,  0.0927,  ..., -0.1522,  0.0247,  0.2887],\n",
        "        [ 0.1365,  0.0957, -0.0611,  ...,  0.0462,  0.0153, -0.2304],\n",
        "        ...,\n",
        "        [ 0.0477,  0.2365, -0.1074,  ...,  0.0071,  0.0043, -0.3155],\n",
        "        [ 0.0067,  0.0112, -0.2509,  ..., -0.0420,  0.0087, -0.0415],\n",
        "        [-0.0426,  0.3102,  0.0678,  ..., -0.0504,  0.0289, -0.3696]],\n",
        "       device='cuda:0', grad_fn=<IndexBackward>)\n",
        "\n",
        "out_forward shape: torch.Size([64, 128])\n",
        "```\n",
        "\n",
        "out_test: \n",
        "```\n",
        "tensor([[ 0.0341, -0.0997, -0.0522,  ..., -0.0227,  0.0251,  0.3660],\n",
        "        [ 0.0488,  0.0952,  0.0927,  ..., -0.1522,  0.0247,  0.2887],\n",
        "        [ 0.1365,  0.0957, -0.0611,  ...,  0.0462,  0.0153, -0.2304],\n",
        "        ...,\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
        "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
        "       device='cuda:0', grad_fn=<SliceBackward>)\n",
        "\n",
        "out_test shape: torch.Size([64, 128])\n",
        "```\n",
        "\n",
        "hidden_forward: \n",
        "\n",
        "```\n",
        "tensor([[ 0.0341, -0.0997, -0.0522,  ..., -0.0227,  0.0251,  0.3660],\n",
        "        [ 0.0488,  0.0952,  0.0927,  ..., -0.1522,  0.0247,  0.2887],\n",
        "        [ 0.1365,  0.0957, -0.0611,  ...,  0.0462,  0.0153, -0.2304],\n",
        "        ...,\n",
        "        [ 0.0477,  0.2365, -0.1074,  ...,  0.0071,  0.0043, -0.3155],\n",
        "        [ 0.0067,  0.0112, -0.2509,  ..., -0.0420,  0.0087, -0.0415],\n",
        "        [-0.0426,  0.3102,  0.0678,  ..., -0.0504,  0.0289, -0.3696]],\n",
        "       device='cuda:0', grad_fn=<SliceBackward>)\n",
        "\n",
        "hidden_forward shape: torch.Size([64, 128])\n",
        "```\n",
        "\n",
        "While `out_test`, which is `output[:, -1, :self.dimension]`, has identical values in the first part of the tensor, the latter parts are all 0's. This  does not occur in every batch. In other batches, all of the values in `out_test` are the same as both `out_forward` and `hidden_forward`. When `batch_first=True` in the LSTM model, the dimensions of output are (N, L, D*H_out), so `output[:, -1, :self.dimension]` should give the last time step of the batch up to the first hidden size dimension (forward). Unsure where this discrepancy comes from. It does seem to make more sense though, given the confusion regarding the last time step of the hidden states from output, to just take the concatenation of `hidden[-2, :, :]` and `hidden[-1, :, :]`.  \n",
        "\n",
        "Since the output has dimensions of (batch size, sequence length, 2* size of the hidden state), it's possible that the 0s are coming from the padded values. In `out_forward`, it is only running the full length of the sequence, and nothing past that. \n",
        "\n",
        "From checking the output tensor, the output lengths and the text lengths, we see the following code output:\n",
        "\n",
        "```\n",
        "Output shape: torch.Size([64, 35, 256])\n",
        "output_len: tensor([35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,\n",
        "        35, 35, 35, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
        "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
        "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34])\n",
        "Text length minus 1: tensor([34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
        "        34, 34, 34, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
        "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
        "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33])\n",
        "```\n",
        "\n",
        "It seems that the output shape uses the maximum text length; however, the output length for each batch shows that it's not always the same text_length for each batch. The sequences are arranged such that they are sorted by text_length in descending order. It's likely then, that the 0s are coming from the the padded values when using `output[:, -1, :self.dimension]` as this does not necessarily get the last timestep. \n",
        "\n",
        "Given the confusion that comes from the output tensor in bidirectional LSTMs, it seems easiest to use `hidden[-2, :, :]` and `hidden[-1, :. :]` to determine the last time steps of the forward and backward passes. Especially when the length of the sequences are not always the same."
      ],
      "metadata": {
        "id": "JI0yEhcRHE80"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rcQOAsAAUqGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lApw79W1Uk0d"
      }
    }
  ]
}